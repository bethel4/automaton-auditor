{
  "rubric_metadata": {
    "rubric_name": "Week 2: The Automaton Auditor Self-Evaluation",
    "grading_target": "Week 2 Auditor Repository & Architectural Report",
    "version": "3.0.0"
  },
  "dimensions": [
    {
      "id": "git_forensic_analysis",
      "name": "Git Forensic Analysis",
      "target_artifact": "github_repo",
      "forensic_instruction": "Run 'git log --oneline --reverse' on the\ncloned repository. Count the total number of commits. Check if the commit\nhistory tells a progression story: Environment Setup -> Tool Engineering ->\nGraph Orchestration. Extract all commit messages and timestamps. Flag if\nthere is a single 'init' commit or a 'bulk upload' pattern with no iterative\ndevelopment.",
      "success_pattern": "More than 3 commits showing clear progression from\nsetup to tool engineering to graph orchestration. Atomic, step-by-step\nhistory with meaningful commit messages.",
      "failure_pattern": "Single 'init' commit or bulk upload of all code at\nonce. No iterative development visible. Timestamps clustered within\nminutes."
    },
    {
      "id": "state_management_rigor",
      "name": "State Management Rigor",
      "target_artifact": "github_repo",
      "forensic_instruction": "Scan for 'src/state.py' or equivalent state\ndefinitions in 'src/graph.py'. Use AST parsing (not regex) to find classes\ninheriting from 'BaseModel' (Pydantic) or 'TypedDict'. Verify that the state\nactively maintains a collection of 'Evidence' objects and a list of\n'JudicialOpinion' objects. Check for the use of 'operator.add' and\n'operator.ior' as state reducers in 'Annotated' type hints to prevent data\noverwriting during parallel execution. Capture the full code snippet of the\ncore 'AgentState' definition.",
      "success_pattern": "'AgentState' uses TypedDict or BaseModel with\nAnnotated reducers. 'Evidence' and 'JudicialOpinion' are Pydantic BaseModel\nclasses with typed fields. Reducers like 'operator.add' (for lists) and\n'operator.ior' (for dicts) are present.",
      "failure_pattern": "Plain Python dicts used for state. No Pydantic\nmodels. No reducers, meaning parallel agents will overwrite each other's\ndata."
    },
    {
      "id": "graph_orchestration",
      "name": "Graph Orchestration Architecture",
      "target_artifact": "github_repo",
      "forensic_instruction": "Scan for the 'StateGraph' builder\ninstantiation in 'src/graph.py'. Use AST parsing to analyze\n'builder.add_edge()' and 'builder.add_conditional_edges()' calls. Determine\nif the Detectives (RepoInvestigator, DocAnalyst, VisionInspector) branch out\nfrom a single node and run concurrently (fan-out). Verify there is a\nsynchronization node ('EvidenceAggregator' or equivalent) that collects all\nevidence before the Judges are invoked (fan-in). Verify the Judges\n(Prosecutor, Defense, TechLead) also fan-out in parallel from the\naggregation node and fan-in before the ChiefJustice. Check for conditional\nedges that handle 'Evidence Missing' or 'Node Failure' scenarios. Capture\nthe specific Python block defining the graph's nodes and edges.",
      "success_pattern": "Two distinct parallel fan-out/fan-in patterns: one\nfor Detectives, one for Judges. Conditional edges handle error states. Graph\nstructure: START -> [Detectives in parallel] -> EvidenceAggregator ->\n[Judges in parallel] -> ChiefJustice -> END.",
      "failure_pattern": "Purely linear flow (RepoInvestigator -> DocAnalyst\n-> Judge -> End). No parallel branches. No synchronization node. No\nconditional edges for error handling."
    },
    {
      "id": "safe_tool_engineering",
      "name": "Safe Tool Engineering",
      "target_artifact": "github_repo",
      "forensic_instruction": "Scan 'src/tools/' for the repository cloning\nlogic. Verify that 'tempfile.TemporaryDirectory()' or equivalent sandboxing\nis used for git clone operations. Check for raw 'os.system()' calls -- these\nare a security violation. Verify that 'subprocess.run()' or equivalent is\nused with proper error handling (capturing stdout/stderr, checking return\ncodes). Ensure the cloned repo path is never the live working directory.\nCheck that git authentication errors are handled gracefully. Capture the\nspecific Python function responsible for executing the repository clone.",
      "success_pattern": "All git operations run inside\n'tempfile.TemporaryDirectory()'. 'subprocess.run()' used with error\nhandling. No raw 'os.system()' calls. Authentication failures caught and\nreported.",
      "failure_pattern": "Raw 'os.system(\"git clone <url>\")' drops code\ninto the live working directory. No error handling around shell commands. No\ninput sanitization on the repo URL."
    },
    {
      "id": "structured_output_enforcement",
      "name": "Structured Output Enforcement",
      "target_artifact": "github_repo",
      "forensic_instruction": "Scan Judge nodes in 'src/nodes/judges.py'.\nVerify that LLMs are invoked using '.with_structured_output()' or\n'.bind_tools()' bound to the Pydantic 'JudicialOpinion' schema. Check that\nthe output includes 'score' (int), 'argument' (str), and 'cited_evidence'\n(list). Verify there is retry logic or error handling if a Judge returns\nfreeform text instead of structured JSON. Capture the code block responsible\nfor querying the Judge LLMs.",
      "success_pattern": "All Judge LLM calls use\n'.with_structured_output(JudicialOpinion)' or equivalent. Retry logic exists\nfor malformed outputs. Output is validated against the Pydantic schema\nbefore being added to state.",
      "failure_pattern": "Judge nodes call LLMs with plain prompts and parse\nfreeform text responses. No Pydantic validation on output. No retry on parse\nfailure."
    },
    {
      "id": "judicial_nuance",
      "name": "Judicial Nuance and Dialectics",
      "target_artifact": "github_repo",
      "forensic_instruction": "Scan 'src/nodes/judges.py' or prompt\ntemplates. Verify that Prosecutor, Defense, and Tech Lead personas have\ndistinct, conflicting system prompts. Compare the three prompts -- if they\nshare more than 50% of text, flag as 'Persona Collusion'. Check if the\nProsecutor prompt includes adversarial language and instructions to look for\ngaps, security flaws, and laziness. Check if the Defense prompt includes\ninstructions to reward effort, intent, and creative workarounds. Check if\nthe Tech Lead prompt focuses on architectural soundness, maintainability,and practical viability. Verify the graph forces all three judges to run in\nparallel on the same evidence for each criterion.",
      "success_pattern": "Three clearly distinct personas with conflicting\nphilosophies. Prompts actively instruct the model to be adversarial\n(Prosecutor), forgiving (Defense), or pragmatic (Tech Lead). Judges produce\ngenuinely different scores and arguments for the same evidence.",
      "failure_pattern": "Single agent acts as 'The Grader' with no persona\nseparation. Or three judges exist but share 90% of prompt text, producing\nnear-identical outputs. Scores are random or purely praise/criticism without\nnuance."
    },
    {
      "id": "chief_justice_synthesis",
      "name": "Chief Justice Synthesis Engine",
      "target_artifact": "github_repo",
      "forensic_instruction": "Scan 'src/nodes/justice.py' for the\nChiefJusticeNode implementation. Verify the conflict resolution uses\nhardcoded deterministic Python logic, not just an LLM prompt. Check for\nthese specific rules: (1) Rule of Security -- if the Prosecutor identifies a\nconfirmed security vulnerability, the score is capped at 3 regardless of\nDefense arguments. (2) Rule of Evidence -- if the Defense claims 'Deep\nMetacognition' but Detective evidence shows the artifact is missing, the\nDefense is overruled. (3) Rule of Functionality -- if the Tech Lead confirms\nthe architecture is modular, this carries the highest weight for the\nArchitecture criterion. Check if score variance > 2 triggers a specific\nre-evaluation rule. Verify the output is a structured Markdown report, not a\nconsole print.",
      "success_pattern": "Deterministic Python if/else logic implementing\nnamed rules (security override, fact supremacy, functionality weight). Score\nvariance triggers specific re-evaluation. Output is a Markdown file with\nExecutive Summary, Criterion Breakdown (with dissent), and Remediation\nPlan.",
      "failure_pattern": "ChiefJustice is just another LLM prompt that\naverages the three judge scores. No hardcoded rules. No dissent summary.\nOutput is console text or unstructured."
    },
    {
      "id": "theoretical_depth",
      "name": "Theoretical Depth (Documentation)",
      "target_artifact": "pdf_report",
      "forensic_instruction": "Search the PDF report for these specific\nterms: 'Dialectical Synthesis', 'Fan-In / Fan-Out', 'Metacognition', 'State\nSynchronization'. Determine if the term appears in a substantive\narchitectural explanation or is just a buzzword dropped in the executive\nsummary. Check if the report explains HOW the architecture executes these\nconcepts, not just that they exist. Flag terms that appear without\nsupporting explanation as 'Keyword Dropping'.",
      "success_pattern": "Terms appear in detailed architectural\nexplanations. The report explains how Dialectical Synthesis is implemented\nvia three parallel judge personas. Fan-In/Fan-Out is tied to specific graph\nedges. Metacognition is connected to the system evaluating its own\nevaluation quality.",
      "failure_pattern": "Terms appear only in the executive summary or\nintroduction. No connection to actual implementation. 'We used Dialectical\nSynthesis' with no explanation of how."
    },
    {
      "id": "report_accuracy",
      "name": "Report Accuracy (Cross-Reference)",
      "target_artifact": "pdf_report",
      "forensic_instruction": "Extract all file paths mentioned in the PDF\nreport (e.g., 'We isolated the AST logic in src/tools/ast_parser.py', 'We\nimplemented parallel Judges in src/nodes/judges.py'). Cross-reference each\nclaimed file path against the evidence collected by the RepoInvestigator.\nBuild two lists: (1) Verified Paths -- files that the report mentions and\nactually exist in the repo. (2) Hallucinated Paths -- files the report\nclaims exist but the RepoInvestigator found no evidence of. Flag any claims\nabout features (e.g., 'We implemented parallel Judges') where the code\nevidence contradicts the claim.",
      "success_pattern": "All file paths mentioned in the report exist in\nthe repo. Feature claims match code evidence. Zero hallucinated paths.",
      "failure_pattern": "Report references files that do not exist. Claims\nparallel execution but code shows linear flow. Multiple hallucinated paths\ndetected."
    },
    {
      "id": "swarm_visual",
      "name": "Architectural Diagram Analysis",
      "target_artifact": "pdf_images",
      "forensic_instruction": "Extract images from the PDF report. Classify\neach diagram: is it an accurate LangGraph State Machine diagram, a sequence\ndiagram, or just generic flowchart boxes? Check if the diagram explicitly\nvisualizes the parallel split: START -> [Detectives in parallel] -> Evidence\nAggregation -> [Prosecutor || Defense || TechLead in parallel] -> Chief\nJustice Synthesis -> END. Verify the diagram distinguishes between parallel\nbranches and sequential steps. Flag diagrams that show a simple linear\npipeline as 'Misleading Architecture Visual'.",
      "success_pattern": "Diagram accurately represents the StateGraph with\nclear parallel branches for both Detectives and Judges. Fan-out and fan-in\npoints are visually distinct. Flow matches the actual code architecture.",
      "failure_pattern": "Generic box-and-arrow diagram with no indication\nof parallelism. Or no diagram present at all. Diagram shows linear flow that\ncontradicts the parallel architecture claimed in the report."
    }
  ],
  "synthesis_rules": {
    "security_override": "Confirmed security flaws (e.g., shell injection in\ngit tools, raw os.system with unsanitized inputs) cap the total score at 3,\noverriding any effort points from the Defense.",
    "fact_supremacy": "Forensic evidence (facts from Detectives) always\noverrules Judicial opinion (interpretation from Judges). If the Defense\nclaims 'Deep Metacognition' but the RepoInvestigator found no supporting\ncode, the Defense is overruled for hallucination.",
    "functionality_weight": "If the Tech Lead confirms the architecture is\nmodular and workable, this carries the highest weight for the 'Graph\nOrchestration Architecture' criterion.",
    "dissent_requirement": "The Chief Justice must summarize why the\nProsecutor and Defense disagreed in the final report. Every criterion with a\nscore variance > 2 must include an explicit dissent explanation.",
    "variance_re_evaluation": "If score variance across the three judges\nexceeds 2 for any criterion (e.g., Prosecutor says 1, Defense says 5),\ntrigger a re-evaluation of the specific evidence cited by each judge before\nrendering the final score."
  }
}
